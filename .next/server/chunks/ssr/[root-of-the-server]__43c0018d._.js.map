{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 207, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/src/ai/genkit.ts"],"sourcesContent":["import {genkit} from 'genkit';\nimport {googleAI} from '@genkit-ai/googleai';\n\nexport const ai = genkit({\n  plugins: [googleAI()],\n  model: 'googleai/gemini-2.0-flash',\n});\n"],"names":[],"mappings":";;;AAAA;AAAA;AACA;AAAA;;;AAEO,MAAM,KAAK,CAAA,GAAA,uIAAA,CAAA,SAAM,AAAD,EAAE;IACvB,SAAS;QAAC,CAAA,GAAA,2KAAA,CAAA,WAAQ,AAAD;KAAI;IACrB,OAAO;AACT","debugId":null}},
    {"offset": {"line": 228, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/src/ai/flows/teach-anything.ts"],"sourcesContent":["\n'use server';\n\n/**\n * @fileOverview A general purpose teaching assistant flow.\n *\n * - teachAnything - A function that provides general teaching assistance.\n * - TeachAnythingInput - The input type for the teachAnything function.\n * - TeachAnythingOutput - The return type for the teachAnything function.\n */\n\nimport {ai} from '@/ai/genkit';\nimport {z} from 'genkit';\n\nconst TeachAnythingInputSchema = z.object({\n  query: z.string().describe('The user\\'s question or request.'),\n});\nexport type TeachAnythingInput = z.infer<typeof TeachAnythingInputSchema>;\n\nconst TeachAnythingOutputSchema = z.object({\n  response: z.string().describe('The AI\\'s response to the user.'),\n});\nexport type TeachAnythingOutput = z.infer<typeof TeachAnythingOutputSchema>;\n\nexport async function teachAnything(input: TeachAnythingInput): Promise<TeachAnythingOutput> {\n  return teachAnythingFlow(input);\n}\n\nconst prompt = ai.definePrompt({\n  name: 'teachAnythingPrompt',\n  input: {schema: TeachAnythingInputSchema},\n  output: {schema: TeachAnythingOutputSchema},\n  prompt: `You are Sahayak, a helpful AI teaching assistant for educators in India. Your goal is to provide concise, practical, and helpful responses to teacher's questions. You can help with lesson planning, classroom management strategies, creating educational content, and student engagement techniques.\n\nUser prompt: {{{query}}}`,\n});\n\nconst teachAnythingFlow = ai.defineFlow(\n  {\n    name: 'teachAnythingFlow',\n    inputSchema: TeachAnythingInputSchema,\n    outputSchema: TeachAnythingOutputSchema,\n  },\n  async input => {\n    const {output} = await prompt(input);\n    return output!;\n  }\n);\n"],"names":[],"mappings":";;;;;AAGA;;;;;;CAMC,GAED;AACA;AAAA;;;;;;AAEA,MAAM,2BAA2B,uIAAA,CAAA,IAAC,CAAC,MAAM,CAAC;IACxC,OAAO,uIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,CAAC;AAC7B;AAGA,MAAM,4BAA4B,uIAAA,CAAA,IAAC,CAAC,MAAM,CAAC;IACzC,UAAU,uIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,CAAC;AAChC;AAGO,eAAe,cAAc,KAAyB;IAC3D,OAAO,kBAAkB;AAC3B;AAEA,MAAM,SAAS,mHAAA,CAAA,KAAE,CAAC,YAAY,CAAC;IAC7B,MAAM;IACN,OAAO;QAAC,QAAQ;IAAwB;IACxC,QAAQ;QAAC,QAAQ;IAAyB;IAC1C,QAAQ,CAAC;;wBAEa,CAAC;AACzB;AAEA,MAAM,oBAAoB,mHAAA,CAAA,KAAE,CAAC,UAAU,CACrC;IACE,MAAM;IACN,aAAa;IACb,cAAc;AAChB,GACA,OAAM;IACJ,MAAM,EAAC,MAAM,EAAC,GAAG,MAAM,OAAO;IAC9B,OAAO;AACT;;;IAtBoB;;AAAA,+OAAA","debugId":null}},
    {"offset": {"line": 343, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/src/ai/flows/speech-to-text.ts"],"sourcesContent":["\n'use server';\n\n/**\n * @fileOverview A Genkit flow for transcribing audio to text using Google Cloud Speech-to-Text API.\n * \n * - transcribeAudio - A function that transcribes audio data.\n * - TranscribeAudioInput - The input type for the transcribeAudio function.\n * - TranscribeAudioOutput - The return type for the transcribeAudio function.\n */\n\nimport { ai } from '@/ai/genkit';\nimport { z } from 'genkit';\nimport { SpeechClient } from '@google-cloud/speech';\n\nconst TranscribeAudioInputSchema = z.object({\n  audioDataUri: z\n    .string()\n    .describe(\n      \"The audio data to transcribe, as a data URI. Expected format: 'data:audio/webm;codecs=opus;base64,<encoded_data>'.\"\n    ),\n  languageCode: z.string().optional().default('en-IN').describe('The language of the audio.'),\n});\nexport type TranscribeAudioInput = z.infer<typeof TranscribeAudioInputSchema>;\n\nconst TranscribeAudioOutputSchema = z.object({\n  transcription: z.string().describe('The transcribed text.'),\n});\nexport type TranscribeAudioOutput = z.infer<typeof TranscribeAudioOutputSchema>;\n\n// Initialize the SpeechClient.\n// The client will automatically use the project's service account credentials\n// when deployed on Firebase App Hosting. For local development, ensure you have\n// authenticated with `gcloud auth application-default login`.\nlet speechClient: SpeechClient | undefined;\ntry {\n  speechClient = new SpeechClient();\n} catch (e) {\n  console.warn(\"SpeechClient failed to initialize. Voice input may not work in local development if ADC is not configured. It will work when deployed.\", e);\n}\n\n\nexport async function transcribeAudio(\n  input: TranscribeAudioInput\n): Promise<TranscribeAudioOutput> {\n  return transcribeAudioFlow(input);\n}\n\nconst transcribeAudioFlow = ai.defineFlow(\n  {\n    name: 'transcribeAudioFlow',\n    inputSchema: TranscribeAudioInputSchema,\n    outputSchema: TranscribeAudioOutputSchema,\n  },\n  async ({ audioDataUri, languageCode }) => {\n    if (!speechClient) {\n        throw new Error(\"SpeechClient not initialized. This feature may only be available when deployed.\");\n    }\n      \n    // The base64 data is the part of the data URI after the comma\n    const audioBytes = audioDataUri.split(',')[1];\n\n    const audio = {\n      content: audioBytes,\n    };\n    \n    // Note: The 'chirp' model used here is part of the v2 API features,\n    // but the Node.js client can access it via the v1p1beta1 endpoint.\n    const config = {\n      model: 'chirp',\n      encoding: 'WEBM_OPUS' as const,\n      sampleRateHertz: 48000, // This must match the client recording settings\n      languageCode: languageCode,\n    };\n\n    const request = {\n      audio: audio,\n      config: config,\n    };\n\n    try {\n      const [response] = await speechClient.recognize(request);\n      const transcription =\n        response.results\n          ?.map((result) => result.alternatives?.[0].transcript)\n          .join('\\n') || '';\n\n      return { transcription };\n    } catch (error) {\n        console.error('Speech-to-text API error:', error);\n        throw new Error('Failed to transcribe audio.');\n    }\n  }\n);\n"],"names":[],"mappings":";;;;;AAGA;;;;;;CAMC,GAED;AACA;AAAA;AACA;;;;;;;AAEA,MAAM,6BAA6B,uIAAA,CAAA,IAAC,CAAC,MAAM,CAAC;IAC1C,cAAc,uIAAA,CAAA,IAAC,CACZ,MAAM,GACN,QAAQ,CACP;IAEJ,cAAc,uIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,OAAO,CAAC,SAAS,QAAQ,CAAC;AAChE;AAGA,MAAM,8BAA8B,uIAAA,CAAA,IAAC,CAAC,MAAM,CAAC;IAC3C,eAAe,uIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,CAAC;AACrC;AAGA,+BAA+B;AAC/B,8EAA8E;AAC9E,gFAAgF;AAChF,8DAA8D;AAC9D,IAAI;AACJ,IAAI;IACF,eAAe,IAAI,oKAAA,CAAA,eAAY;AACjC,EAAE,OAAO,GAAG;IACV,QAAQ,IAAI,CAAC,0IAA0I;AACzJ;AAGO,eAAe,gBACpB,KAA2B;IAE3B,OAAO,oBAAoB;AAC7B;AAEA,MAAM,sBAAsB,mHAAA,CAAA,KAAE,CAAC,UAAU,CACvC;IACE,MAAM;IACN,aAAa;IACb,cAAc;AAChB,GACA,OAAO,EAAE,YAAY,EAAE,YAAY,EAAE;IACnC,IAAI,CAAC,cAAc;QACf,MAAM,IAAI,MAAM;IACpB;IAEA,8DAA8D;IAC9D,MAAM,aAAa,aAAa,KAAK,CAAC,IAAI,CAAC,EAAE;IAE7C,MAAM,QAAQ;QACZ,SAAS;IACX;IAEA,oEAAoE;IACpE,mEAAmE;IACnE,MAAM,SAAS;QACb,OAAO;QACP,UAAU;QACV,iBAAiB;QACjB,cAAc;IAChB;IAEA,MAAM,UAAU;QACd,OAAO;QACP,QAAQ;IACV;IAEA,IAAI;QACF,MAAM,CAAC,SAAS,GAAG,MAAM,aAAa,SAAS,CAAC;QAChD,MAAM,gBACJ,SAAS,OAAO,EACZ,IAAI,CAAC,SAAW,OAAO,YAAY,EAAE,CAAC,EAAE,CAAC,YAC1C,KAAK,SAAS;QAEnB,OAAO;YAAE;QAAc;IACzB,EAAE,OAAO,OAAO;QACZ,QAAQ,KAAK,CAAC,6BAA6B;QAC3C,MAAM,IAAI,MAAM;IACpB;AACF;;;IAlDoB;;AAAA,+OAAA","debugId":null}},
    {"offset": {"line": 431, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/src/ai/flows/text-to-speech.ts"],"sourcesContent":["\n'use server';\n\n/**\n * @fileOverview A Genkit flow for synthesizing text into speech.\n * \n * - synthesizeSpeech - A function that converts text to audio.\n * - SynthesizeSpeechInput - The input type for the synthesizeSpeech function.\n * - SynthesizeSpeechOutput - The return type for the synthesizeSpeech function.\n */\n\nimport { ai } from '@/ai/genkit';\nimport { z } from 'genkit';\nimport wav from 'wav';\nimport { googleAI } from '@genkit-ai/googleai';\n\n\nconst SynthesizeSpeechInputSchema = z.object({\n  text: z.string().describe('The text to synthesize.'),\n  languageCode: z.string().optional().default('en-IN').describe('The language for the speech.'),\n});\nexport type SynthesizeSpeechInput = z.infer<typeof SynthesizeSpeechInputSchema>;\n\nconst SynthesizeSpeechOutputSchema = z.object({\n    audioDataUri: z.string().describe(\"The synthesized audio as a data URI. Expected format: 'data:audio/wav;base64,<encoded_data>'.\"),\n});\nexport type SynthesizeSpeechOutput = z.infer<typeof SynthesizeSpeechOutputSchema>;\n\n\nexport async function synthesizeSpeech(\n  input: SynthesizeSpeechInput\n): Promise<SynthesizeSpeechOutput> {\n  return synthesizeSpeechFlow(input);\n}\n\nasync function toWav(\n  pcmData: Buffer,\n  channels = 1,\n  rate = 24000,\n  sampleWidth = 2\n): Promise<string> {\n  return new Promise((resolve, reject) => {\n    const writer = new wav.Writer({\n      channels,\n      sampleRate: rate,\n      bitDepth: sampleWidth * 8,\n    });\n\n    let bufs: any[] = [];\n    writer.on('error', reject);\n    writer.on('data', function (d) {\n      bufs.push(d);\n    });\n    writer.on('end', function () {\n      resolve(Buffer.concat(bufs).toString('base64'));\n    });\n\n    writer.write(pcmData);\n    writer.end();\n  });\n}\n\nconst synthesizeSpeechFlow = ai.defineFlow(\n  {\n    name: 'synthesizeSpeechFlow',\n    inputSchema: SynthesizeSpeechInputSchema,\n    outputSchema: SynthesizeSpeechOutputSchema,\n  },\n  async ({ text }) => {\n    if (!text) {\n        return { audioDataUri: '' };\n    }\n\n    try {\n        const { media } = await ai.generate({\n            model: googleAI.model('gemini-2.5-flash-preview-tts'),\n            config: {\n                responseModalities: ['AUDIO'],\n                speechConfig: {\n                voiceConfig: {\n                    prebuiltVoiceConfig: { voiceName: 'Algenib' },\n                },\n                },\n            },\n            prompt: text.substring(0, 5000), // Enforce a character limit\n        });\n\n        if (!media) {\n            throw new Error('No audio media returned from TTS model.');\n        }\n\n        const audioBuffer = Buffer.from(\n            media.url.substring(media.url.indexOf(',') + 1),\n            'base64'\n        );\n        \n        const wavBase64 = await toWav(audioBuffer);\n\n        return {\n            audioDataUri: 'data:audio/wav;base64,' + wavBase64,\n        };\n    } catch (error) {\n        console.error('Text-to-speech API error:', error);\n        throw new Error('Failed to synthesize speech.');\n    }\n  }\n);\n"],"names":[],"mappings":";;;;;AAGA;;;;;;CAMC,GAED;AACA;AAAA;AACA;AACA;AAAA;;;;;;;;AAGA,MAAM,8BAA8B,uIAAA,CAAA,IAAC,CAAC,MAAM,CAAC;IAC3C,MAAM,uIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,CAAC;IAC1B,cAAc,uIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,OAAO,CAAC,SAAS,QAAQ,CAAC;AAChE;AAGA,MAAM,+BAA+B,uIAAA,CAAA,IAAC,CAAC,MAAM,CAAC;IAC1C,cAAc,uIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,CAAC;AACtC;AAIO,eAAe,iBACpB,KAA4B;IAE5B,OAAO,qBAAqB;AAC9B;AAEA,eAAe,MACb,OAAe,EACf,WAAW,CAAC,EACZ,OAAO,KAAK,EACZ,cAAc,CAAC;IAEf,OAAO,IAAI,QAAQ,CAAC,SAAS;QAC3B,MAAM,SAAS,IAAI,4HAAA,CAAA,UAAG,CAAC,MAAM,CAAC;YAC5B;YACA,YAAY;YACZ,UAAU,cAAc;QAC1B;QAEA,IAAI,OAAc,EAAE;QACpB,OAAO,EAAE,CAAC,SAAS;QACnB,OAAO,EAAE,CAAC,QAAQ,SAAU,CAAC;YAC3B,KAAK,IAAI,CAAC;QACZ;QACA,OAAO,EAAE,CAAC,OAAO;YACf,QAAQ,OAAO,MAAM,CAAC,MAAM,QAAQ,CAAC;QACvC;QAEA,OAAO,KAAK,CAAC;QACb,OAAO,GAAG;IACZ;AACF;AAEA,MAAM,uBAAuB,mHAAA,CAAA,KAAE,CAAC,UAAU,CACxC;IACE,MAAM;IACN,aAAa;IACb,cAAc;AAChB,GACA,OAAO,EAAE,IAAI,EAAE;IACb,IAAI,CAAC,MAAM;QACP,OAAO;YAAE,cAAc;QAAG;IAC9B;IAEA,IAAI;QACA,MAAM,EAAE,KAAK,EAAE,GAAG,MAAM,mHAAA,CAAA,KAAE,CAAC,QAAQ,CAAC;YAChC,OAAO,2KAAA,CAAA,WAAQ,CAAC,KAAK,CAAC;YACtB,QAAQ;gBACJ,oBAAoB;oBAAC;iBAAQ;gBAC7B,cAAc;oBACd,aAAa;wBACT,qBAAqB;4BAAE,WAAW;wBAAU;oBAChD;gBACA;YACJ;YACA,QAAQ,KAAK,SAAS,CAAC,GAAG;QAC9B;QAEA,IAAI,CAAC,OAAO;YACR,MAAM,IAAI,MAAM;QACpB;QAEA,MAAM,cAAc,OAAO,IAAI,CAC3B,MAAM,GAAG,CAAC,SAAS,CAAC,MAAM,GAAG,CAAC,OAAO,CAAC,OAAO,IAC7C;QAGJ,MAAM,YAAY,MAAM,MAAM;QAE9B,OAAO;YACH,cAAc,2BAA2B;QAC7C;IACJ,EAAE,OAAO,OAAO;QACZ,QAAQ,KAAK,CAAC,6BAA6B;QAC3C,MAAM,IAAI,MAAM;IACpB;AACF;;;IA5EoB;;AAAA,+OAAA","debugId":null}},
    {"offset": {"line": 535, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/.next-internal/server/app/page/actions.js%20%28server%20actions%20loader%29"],"sourcesContent":["export {teachAnything as '40f4e56a2b2c9e62cbc0c3320c3c6d55217611e928'} from 'ACTIONS_MODULE0'\nexport {transcribeAudio as '40c665cfb49e988d1b420756d26c556b177fc46e64'} from 'ACTIONS_MODULE1'\nexport {synthesizeSpeech as '40df10d4f37c0a9fcd0bccfa1a8013c58464b0b4d1'} from 'ACTIONS_MODULE2'\n"],"names":[],"mappings":";AAAA;AACA;AACA","debugId":null}},
    {"offset": {"line": 599, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/src/app/dashboard-client.tsx/proxy.mjs"],"sourcesContent":["import { registerClientReference } from \"react-server-dom-turbopack/server.edge\";\nexport const DashboardClient = registerClientReference(\n    function() { throw new Error(\"Attempted to call DashboardClient() from the server but DashboardClient is on the client. It's not possible to invoke a client function from the server, it can only be rendered as a Component or passed to props of a Client Component.\"); },\n    \"[project]/src/app/dashboard-client.tsx <module evaluation>\",\n    \"DashboardClient\",\n);\n"],"names":[],"mappings":";;;AAAA;;AACO,MAAM,kBAAkB,CAAA,GAAA,qPAAA,CAAA,0BAAuB,AAAD,EACjD;IAAa,MAAM,IAAI,MAAM;AAA8O,GAC3Q,8DACA","debugId":null}},
    {"offset": {"line": 613, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/src/app/dashboard-client.tsx/proxy.mjs"],"sourcesContent":["import { registerClientReference } from \"react-server-dom-turbopack/server.edge\";\nexport const DashboardClient = registerClientReference(\n    function() { throw new Error(\"Attempted to call DashboardClient() from the server but DashboardClient is on the client. It's not possible to invoke a client function from the server, it can only be rendered as a Component or passed to props of a Client Component.\"); },\n    \"[project]/src/app/dashboard-client.tsx\",\n    \"DashboardClient\",\n);\n"],"names":[],"mappings":";;;AAAA;;AACO,MAAM,kBAAkB,CAAA,GAAA,qPAAA,CAAA,0BAAuB,AAAD,EACjD;IAAa,MAAM,IAAI,MAAM;AAA8O,GAC3Q,0CACA","debugId":null}},
    {"offset": {"line": 627, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":"","debugId":null}},
    {"offset": {"line": 637, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/src/app/page.tsx"],"sourcesContent":["\nimport { DashboardClient } from \"@/app/dashboard-client\";\n\nexport default function Home() {\n  return (\n    <div className=\"flex flex-col h-[calc(100vh-8rem)]\">\n      <DashboardClient />\n    </div>\n  );\n}\n"],"names":[],"mappings":";;;;AACA;;;AAEe,SAAS;IACtB,qBACE,8OAAC;QAAI,WAAU;kBACb,cAAA,8OAAC,kIAAA,CAAA,kBAAe;;;;;;;;;;AAGtB","debugId":null}}]
}