{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 7, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/src/ai/flows/speech-to-text.ts"],"sourcesContent":["\n'use server';\n\n/**\n * @fileOverview A Genkit flow for transcribing audio to text using Google Cloud Speech-to-Text API.\n * \n * - transcribeAudio - A function that transcribes audio data.\n * - TranscribeAudioInput - The input type for the transcribeAudio function.\n * - TranscribeAudioOutput - The return type for the transcribeAudio function.\n */\n\nimport { ai } from '@/ai/genkit';\nimport { z } from 'genkit';\nimport { SpeechClient } from '@google-cloud/speech';\n\nconst TranscribeAudioInputSchema = z.object({\n  audioDataUri: z\n    .string()\n    .describe(\n      \"The audio data to transcribe, as a data URI. Expected format: 'data:audio/webm;codecs=opus;base64,<encoded_data>'.\"\n    ),\n  languageCode: z.string().optional().default('en-IN').describe('The language of the audio.'),\n});\nexport type TranscribeAudioInput = z.infer<typeof TranscribeAudioInputSchema>;\n\nconst TranscribeAudioOutputSchema = z.object({\n  transcription: z.string().describe('The transcribed text.'),\n});\nexport type TranscribeAudioOutput = z.infer<typeof TranscribeAudioOutputSchema>;\n\n// Initialize the SpeechClient.\n// The client will automatically use the project's service account credentials\n// when deployed on Firebase App Hosting. For local development, ensure you have\n// authenticated with `gcloud auth application-default login`.\nlet speechClient: SpeechClient | undefined;\ntry {\n  speechClient = new SpeechClient();\n} catch (e) {\n  console.warn(\"SpeechClient failed to initialize. Voice input may not work in local development if ADC is not configured. It will work when deployed.\", e);\n}\n\n\nexport async function transcribeAudio(\n  input: TranscribeAudioInput\n): Promise<TranscribeAudioOutput> {\n  return transcribeAudioFlow(input);\n}\n\nconst transcribeAudioFlow = ai.defineFlow(\n  {\n    name: 'transcribeAudioFlow',\n    inputSchema: TranscribeAudioInputSchema,\n    outputSchema: TranscribeAudioOutputSchema,\n  },\n  async ({ audioDataUri, languageCode }) => {\n    if (!speechClient) {\n        throw new Error(\"SpeechClient not initialized. This feature may only be available when deployed.\");\n    }\n      \n    // The base64 data is the part of the data URI after the comma\n    const audioBytes = audioDataUri.split(',')[1];\n\n    const audio = {\n      content: audioBytes,\n    };\n    \n    // Note: The 'chirp' model used here is part of the v2 API features,\n    // but the Node.js client can access it via the v1p1beta1 endpoint.\n    const config = {\n      model: 'chirp',\n      encoding: 'WEBM_OPUS' as const,\n      sampleRateHertz: 48000, // This must match the client recording settings\n      languageCode: languageCode,\n    };\n\n    const request = {\n      audio: audio,\n      config: config,\n    };\n\n    try {\n      const [response] = await speechClient.recognize(request);\n      const transcription =\n        response.results\n          ?.map((result) => result.alternatives?.[0].transcript)\n          .join('\\n') || '';\n\n      return { transcription };\n    } catch (error) {\n        console.error('Speech-to-text API error:', error);\n        throw new Error('Failed to transcribe audio.');\n    }\n  }\n);\n"],"names":[],"mappings":";;;;;;IA0CsB,kBAAA,WAAA,GAAA,CAAA,GAAA,yNAAA,CAAA,wBAAA,EAAA,8CAAA,yNAAA,CAAA,aAAA,EAAA,KAAA,GAAA,yNAAA,CAAA,mBAAA,EAAA","debugId":null}},
    {"offset": {"line": 23, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/src/components/voice-input.tsx"],"sourcesContent":["\n\"use client\";\n\nimport { useState } from 'react';\nimport { useReactMediaRecorder } from 'react-media-recorder';\nimport { Button } from '@/components/ui/button';\nimport { Mic, Loader2, StopCircle } from 'lucide-react';\nimport { useToast } from '@/hooks/use-toast';\nimport { transcribeAudio } from '@/ai/flows/speech-to-text';\n\ninterface VoiceInputProps {\n  onTranscript: (transcript: string) => void;\n  language?: string;\n  className?: string;\n}\n\nconst fileToDataUri = (blob: Blob): Promise<string> => {\n    return new Promise((resolve, reject) => {\n      const reader = new FileReader();\n      reader.onloadend = () => {\n        if (typeof reader.result === 'string') {\n          resolve(reader.result);\n        } else {\n          reject(new Error('Failed to convert blob to Data URI'));\n        }\n      };\n      reader.onerror = reject;\n      reader.readAsDataURL(blob);\n    });\n};\n\nexport function VoiceInput({ onTranscript, language = 'en-IN', className }: VoiceInputProps) {\n  const [isTranscribing, setIsTranscribing] = useState(false);\n  const { toast } = useToast();\n\n  const handleAudioStop = async (blobUrl: string, blob: Blob) => {\n    setIsTranscribing(true);\n    try {\n      const audioDataUri = await fileToDataUri(blob);\n      \n      const result = await transcribeAudio({ audioDataUri, languageCode: language });\n      \n      if (result.transcription) {\n        onTranscript(result.transcription);\n      } else {\n        toast({\n          variant: 'destructive',\n          title: 'Transcription Failed',\n          description: 'Could not understand audio. Please try again.',\n        });\n      }\n    } catch (error) {\n      console.error('Voice input error:', error);\n      toast({\n        variant: 'destructive',\n        title: 'Voice Error',\n        description: 'Failed to process audio. Please check permissions and try again.',\n      });\n    } finally {\n      setIsTranscribing(false);\n      clearBlobUrl();\n    }\n  };\n\n  const {\n    status,\n    startRecording,\n    stopRecording,\n    clearBlobUrl,\n  } = useReactMediaRecorder({ \n    audio: {\n        channelCount: 1,\n    },\n    onStop: handleAudioStop,\n    onPermissionDenied: () => {\n        toast({\n            variant: \"destructive\",\n            title: \"Microphone Permission Denied\",\n            description: \"Please allow microphone access in your browser settings to use this feature.\"\n        })\n    }\n  });\n\n  return (\n    <Button\n      type=\"button\"\n      size=\"icon\"\n      className={`rounded-full ${className}`}\n      onClick={status === 'recording' ? stopRecording : startRecording}\n      disabled={isTranscribing}\n    >\n      {isTranscribing ? (\n        <Loader2 className=\"animate-spin\" />\n      ) : status === 'recording' ? (\n        <StopCircle className=\"text-destructive\" />\n      ) : (\n        <Mic />\n      )}\n    </Button>\n  );\n};\n"],"names":[],"mappings":";;;;AAGA;AACA;AACA;AACA;AAAA;AAAA;AACA;AACA;;;AAPA;;;;;;;AAeA,MAAM,gBAAgB,CAAC;IACnB,OAAO,IAAI,QAAQ,CAAC,SAAS;QAC3B,MAAM,SAAS,IAAI;QACnB,OAAO,SAAS,GAAG;YACjB,IAAI,OAAO,OAAO,MAAM,KAAK,UAAU;gBACrC,QAAQ,OAAO,MAAM;YACvB,OAAO;gBACL,OAAO,IAAI,MAAM;YACnB;QACF;QACA,OAAO,OAAO,GAAG;QACjB,OAAO,aAAa,CAAC;IACvB;AACJ;AAEO,SAAS,WAAW,EAAE,YAAY,EAAE,WAAW,OAAO,EAAE,SAAS,EAAmB;;IACzF,MAAM,CAAC,gBAAgB,kBAAkB,GAAG,CAAA,GAAA,6JAAA,CAAA,WAAQ,AAAD,EAAE;IACrD,MAAM,EAAE,KAAK,EAAE,GAAG,CAAA,GAAA,+HAAA,CAAA,WAAQ,AAAD;IAEzB,MAAM,kBAAkB,OAAO,SAAiB;QAC9C,kBAAkB;QAClB,IAAI;YACF,MAAM,eAAe,MAAM,cAAc;YAEzC,MAAM,SAAS,MAAM,CAAA,GAAA,6JAAA,CAAA,kBAAe,AAAD,EAAE;gBAAE;gBAAc,cAAc;YAAS;YAE5E,IAAI,OAAO,aAAa,EAAE;gBACxB,aAAa,OAAO,aAAa;YACnC,OAAO;gBACL,MAAM;oBACJ,SAAS;oBACT,OAAO;oBACP,aAAa;gBACf;YACF;QACF,EAAE,OAAO,OAAO;YACd,QAAQ,KAAK,CAAC,sBAAsB;YACpC,MAAM;gBACJ,SAAS;gBACT,OAAO;gBACP,aAAa;YACf;QACF,SAAU;YACR,kBAAkB;YAClB;QACF;IACF;IAEA,MAAM,EACJ,MAAM,EACN,cAAc,EACd,aAAa,EACb,YAAY,EACb,GAAG,CAAA,GAAA,sJAAA,CAAA,wBAAqB,AAAD,EAAE;QACxB,OAAO;YACH,cAAc;QAClB;QACA,QAAQ;QACR,kBAAkB;gDAAE;gBAChB,MAAM;oBACF,SAAS;oBACT,OAAO;oBACP,aAAa;gBACjB;YACJ;;IACF;IAEA,qBACE,6LAAC,qIAAA,CAAA,SAAM;QACL,MAAK;QACL,MAAK;QACL,WAAW,CAAC,aAAa,EAAE,WAAW;QACtC,SAAS,WAAW,cAAc,gBAAgB;QAClD,UAAU;kBAET,+BACC,6LAAC,oNAAA,CAAA,UAAO;YAAC,WAAU;;;;;mBACjB,WAAW,4BACb,6LAAC,qNAAA,CAAA,aAAU;YAAC,WAAU;;;;;iCAEtB,6LAAC,mMAAA,CAAA,MAAG;;;;;;;;;;AAIZ;GArEgB;;QAEI,+HAAA,CAAA,WAAQ;QAoCtB,sJAAA,CAAA,wBAAqB;;;KAtCX","debugId":null}}]
}